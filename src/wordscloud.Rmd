---
title: "2&4words"
author: "I DONT HAVE FRIENDS LA"
date: "2019/12/26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(stringr)
library(jiebaR)
library(tidytext)
library(dplyr)
library(wordcloud2)
```

# 最近期文字雲
## 二字
```{r}
#1-1500
setwd("C:/Users/Jouan/Desktop/R/期末專題/")
fps1 <- list.files("GS66/15", full.names = T)
# Initialize jiebaR and the dictionary
seg1<-worker(user="keywords.txt",stop_word = "stopwords.txt")

# Initialize empty vector to use in for loop
contents1 <- vector("character", length(fps1))

for (i in seq_along(fps1)) {
  # Read post from file
  post1 <- readLines(fps1[i], encoding = "UTF-8")
  
  # Segment post
  segged1 <- segment(post1, seg1)
  contents1[i] <- paste(segged1, collapse = " ")
}

# Combine results into a df
to1500 <- tibble::tibble(id = seq_along(contents1), content = contents1)

#詞頻
to1500<-to1500 %>%
  unnest_tokens(output="word",
                input="content", 
                token="regex",
                pattern = " ")  

#計算出現詞彙次數
topicword_1500<-to1500 %>% 
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))


#製作年份最新文字雲
wordcloud2(topicword_1500, color = "random-light",fontFamily = "微軟正黑體", backgroundColor = "black")#顏色和背景
```

## 四字
```{r}
to1500_idiom<-to1500 %>% filter(str_detect(to1500$word, ".{4}"))

#計算出現詞彙次數
topicword_1500_idiom<-to1500_idiom %>% 
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

#製作年份最新文字雲
wordcloud2(topicword_1500_idiom, color = "random-light",fontFamily = "微軟正黑體", backgroundColor = "black")
```


# 次近期文字雲
## 二字
```{r}
#1500to3000行
fps2 <- list.files("GS66/30", full.names = T)
# Initialize jiebaR and the dictionary
seg2<-worker(user="keywords.txt",stop_word = "stopwords.txt")

# Initialize empty vector to use in for loop
contents2 <- vector("character", length(fps2))

for (i in seq_along(fps2)) {
  # Read post from file
  post2 <- readLines(fps2[i], encoding = "UTF-8")
  
  # Segment post
  segged2 <- segment(post2, seg2)
  contents2[i] <- paste(segged2, collapse = " ")
}

# Combine results into a df
to4500 <- tibble::tibble(id = seq_along(contents2), content = contents2)

#詞頻
to4500<-to4500 %>%
  unnest_tokens(output="word", 
                input="content",
                token="regex",
                pattern = " ") 

#計算出現詞彙次數
topicword_4500<-to4500 %>% 
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

#製作年份次新的文字雲
wordcloud2(topicword_4500, color = "random-light",fontFamily = "微軟正黑體", backgroundColor = "black")#顏色和背景
```

##四字
```{r}
to4500_idiom<-to4500 %>% filter(str_detect(to4500$word, ".{4}"))

#計算出現詞彙次數
topicword_4500_idiom<-to4500_idiom %>% 
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

#製作年份次新文字雲
wordcloud2(topicword_4500_idiom, color = "random-light",fontFamily = "微軟正黑體", backgroundColor = "black")
```


# 次舊文字雲
## 二字
```{r}
#3001to4500行
fps3 <- list.files("GS66/45", full.names = T)
# Initialize jiebaR and the dictionary
seg3<-worker(user="keywords.txt",stop_word = "stopwords.txt")

# Initialize empty vector to use in for loop
contents3 <- vector("character", length(fps3))

for (i in seq_along(fps3)) {
  # Read post from file
  post3 <- readLines(fps3[i], encoding = "UTF-8")
  
  # Segment post
  segged3 <- segment(post3, seg3)
  contents3[i] <- paste(segged3, collapse = " ")
}

# Combine results into a df
to6000 <- tibble::tibble(id = seq_along(contents3), content = contents3)

#詞頻
to6000<-to6000 %>%
  unnest_tokens(output="word", 
                input="content", 
                token="regex",
                pattern = " ") 

#計算出現詞彙次數
topicword_6000<-to6000 %>% 
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

#製作年份次舊的文字雲
wordcloud2(topicword_6000, color = "random-light",fontFamily = "微軟正黑體", backgroundColor = "black")
```

## 四字
```{r}
to6000_idiom<-to6000 %>% filter(str_detect(to6000$word, ".{4}"))

#計算出現詞彙次數
topicword_6000_idiom<-to6000_idiom %>% 
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

#製作年份次舊文字雲
wordcloud2(topicword_6000_idiom, color = "random-light",fontFamily = "微軟正黑體", backgroundColor = "black")
```


# 最舊文字雲
## 二字
```{r}
#to end
fps4 <- list.files("GS66/end", full.names = T)
# Initialize jiebaR and the dictionary
seg4<-worker(user="keywords.txt",stop_word = "stopwords.txt")

# Initialize empty vector to use in for loop
contents4 <- vector("character", length(fps4))

for (i in seq_along(fps4)) {
  # Read post from file
  post4 <- readLines(fps4[i], encoding = "UTF-8")
  
  # Segment post
  segged4 <- segment(post4, seg4)
  contents4[i] <- paste(segged4, collapse = " ")
}

# Combine results into a df
end<- tibble::tibble(id = seq_along(contents4), content = contents4)

#詞頻
end<-end %>%
  unnest_tokens(output="word",  
                input="content",  
                token="regex",
                pattern = " ")

#計算出現詞彙次數
topicword_end<-end %>% 
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

#製作年份最舊的文字雲
wordcloud2(topicword_end, color = "random-light",fontFamily = "微軟正黑體", backgroundColor = "black")
```

## 四字
```{r}
end_idiom<-end%>% filter(str_detect(end$word, ".{4}"))

#計算出現詞彙次數
topicword_end_idiom<-end_idiom %>% 
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

#製作年份最舊文字雲
wordcloud2(topicword_end_idiom, color = "random-light",fontFamily = "微軟正黑體", backgroundColor = "black")
```

```{r}
fps01 <- list.files("TW", full.names = T)
# Initialize jiebaR and the dictionary
seg01<-worker(user="keywordsm.txt",stop_word = "stopwordsm.txt")

# Initialize empty vector to use in for loop
contents01 <- vector("character", length(fps01))

for (i in seq_along(fps01)) {
  # Read post from file
  post01 <- readLines(fps01[i], encoding = "UTF-8")
  
  # Segment post
  segged01 <- segment(post01, seg01)
  contents01[i] <- paste(segged01, collapse = " ")
}

# Combine results into a df
docs_df01 <- tibble::tibble(id = seq_along(contents01), content = contents01)
```


## 製作臺灣外交部二字+文字雲
```{r}
#詞頻
taiwan<-docs_df01 %>%
  unnest_tokens(output="word",  
                input="content",  
                token="regex",
                pattern = " ")

taiwan2<-taiwan %>% filter(str_detect(taiwan$word, ".{2}"))

#計算出現詞彙次數
topicword_taiwan2<-taiwan2 %>% 
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

#製作文字雲
wordcloud2(topicword_taiwan2, color = "random-light",fontFamily = "微軟正黑體", backgroundColor = "black")
```

## 製作臺灣外交部四字+文字雲
```{r}
taiwan4<-taiwan %>% filter(str_detect(taiwan$word, ".{4}+"))

#計算出現詞彙次數
topicword_taiwan4<-taiwan4 %>% 
  group_by(word) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

#製作文字雲
wordcloud2(topicword_taiwan4, color = "random-light",fontFamily = "微軟正黑體", backgroundColor = "black")
```